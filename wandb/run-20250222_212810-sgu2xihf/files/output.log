  0%|                                                                                                                     | 0/1724 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[2;36m[2025-02-22 21:28:27][0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m02[0m-[1;36m22[0m [1;92m21:28:27[0m[1m][0m - ERROR - rich - Training failed: CUDA out of memory. Tried to allocate     ]8;id=231148;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=471029;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#367\[2m367[0m]8;;\
[2;36m                      [0m         [1;36m768.00[0m MiB. GPU [1;36m0[0m has a total capacity of [1;36m4.00[0m GiB of which [1;36m0[0m bytes is free. Including            [2m                  [0m
[2;36m                      [0m         non-PyTorch memory, this process has [1;36m17179869184.00[0m GiB memory in use. Of the allocated memory    [2m                  [0m
[2;36m                      [0m         [1;36m4.73[0m GiB is allocated by PyTorch, and [1;36m478.08[0m MiB is reserved by PyTorch but unallocated. If       [2m                  [0m
[2;36m                      [0m         reserved but unallocated memory is large try setting                                              [2m                  [0m
[2;36m                      [0m         [33mPYTORCH_CUDA_ALLOC_CONF[0m=[35mexpandable_segments[0m:[3;92mTrue[0m to avoid fragmentation.  See documentation for   [2m                  [0m
[2;36m                      [0m         Memory Management  [1m([0m[4;94mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables[0m[4;94m)[0m        [2m                  [0m
[2;36m                     [0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m02[0m-[1;36m22[0m [1;92m21:28:27[0m[1m][0m - ERROR - rich - Unhandled exception: CUDA out of memory. Tried to allocate ]8;id=795667;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=844962;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#375\[2m375[0m]8;;\
[2;36m                      [0m         [1;36m768.00[0m MiB. GPU [1;36m0[0m has a total capacity of [1;36m4.00[0m GiB of which [1;36m0[0m bytes is free. Including            [2m                  [0m
[2;36m                      [0m         non-PyTorch memory, this process has [1;36m17179869184.00[0m GiB memory in use. Of the allocated memory    [2m                  [0m
[2;36m                      [0m         [1;36m4.73[0m GiB is allocated by PyTorch, and [1;36m478.08[0m MiB is reserved by PyTorch but unallocated. If       [2m                  [0m
[2;36m                      [0m         reserved but unallocated memory is large try setting                                              [2m                  [0m
[2;36m                      [0m         [33mPYTORCH_CUDA_ALLOC_CONF[0m=[35mexpandable_segments[0m:[3;92mTrue[0m to avoid fragmentation.  See documentation for   [2m                  [0m
[2;36m                      [0m         Memory Management  [1m([0m[4;94mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables[0m[4;94m)[0m        [2m                  [0m
[2;36m                      [0m         Traceback [1m([0mmost recent call last[1m)[0m:                                                                [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m372[0m, in [1m<[0m[1;95mmodule[0m[1m>[0m  [2m                  [0m
[2;36m                      [0m             [1;35mtrain[0m[1m([0m[1m)[0m                                                                                       [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m347[0m, in train     [2m                  [0m
[2;36m                      [0m             train_result = [1;35mtrainer.train[0m[1m([0m[1m)[0m                                                                [2m                  [0m
[2;36m                      [0m                            ^^^^^^^^^^^^^^^                                                                [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.p[0m [2m                  [0m
[2;36m                      [0m         [32my"[0m, line [1;36m2171[0m, in train                                                                           [2m                  [0m
[2;36m                      [0m             return [1;35minner_training_loop[0m[1m([0m                                                                   [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^                                                                   [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.p[0m [2m                  [0m
[2;36m                      [0m         [32my"[0m, line [1;36m2531[0m, in _inner_training_loop                                                            [2m                  [0m
[2;36m                      [0m             tr_loss_step = [1;35mself.training_step[0m[1m([0mmodel, inputs, num_items_in_batch[1m)[0m                          [2m                  [0m
[2;36m                      [0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                          [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.p[0m [2m                  [0m
[2;36m                      [0m         [32my"[0m, line [1;36m3675[0m, in training_step                                                                   [2m                  [0m
[2;36m                      [0m             loss = [1;35mself.compute_loss[0m[1m([0mmodel, inputs, [33mnum_items_in_batch[0m=[35mnum_items_in_batch[0m[1m)[0m                [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.p[0m [2m                  [0m
[2;36m                      [0m         [32my"[0m, line [1;36m3731[0m, in compute_loss                                                                    [2m                  [0m
[2;36m                      [0m             outputs = [1;35mmodel[0m[1m([0m**inputs[1m)[0m                                                                     [2m                  [0m
[2;36m                      [0m                       ^^^^^^^^^^^^^^^                                                                     [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/modul[0m [2m                  [0m
[2;36m                      [0m         [32me.py"[0m, line [1;36m1736[0m, in _wrapped_call_impl                                                           [2m                  [0m
[2;36m                      [0m             return [1;35mself._call_impl[0m[1m([0m*args, **kwargs[1m)[0m                                                       [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                       [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/modul[0m [2m                  [0m
[2;36m                      [0m         [32me.py"[0m, line [1;36m1747[0m, in _call_impl                                                                   [2m                  [0m
[2;36m                      [0m             return [1;35mforward_call[0m[1m([0m*args, **kwargs[1m)[0m                                                          [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                          [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/utils/opera[0m [2m                  [0m
[2;36m                      [0m         [32mtions.py"[0m, line [1;36m819[0m, in forward                                                                   [2m                  [0m
[2;36m                      [0m             return [1;35mmodel_forward[0m[1m([0m*args, **kwargs[1m)[0m                                                         [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                         [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/utils/opera[0m [2m                  [0m
[2;36m                      [0m         [32mtions.py"[0m, line [1;36m807[0m, in __call__                                                                  [2m                  [0m
[2;36m                      [0m             return [1;35mconvert_to_fp32[0m[1m([0m[1;35mself.model_forward[0m[1m([0m*args, **kwargs[1m)[0m[1m)[0m                                   [2m                  [0m
[2;36m                      [0m                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                    [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/amp/autocast_mod[0m [2m                  [0m
[2;36m                      [0m         [32me.py"[0m, line [1;36m44[0m, in decorate_autocast                                                              [2m                  [0m
[2;36m                      [0m             return [1;35mfunc[0m[1m([0m*args, **kwargs[1m)[0m                                                                  [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^                                                                  [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/peft_model.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m1003[0m, in forward                                                                             [2m                  [0m
[2;36m                      [0m             return [1;35mself.base_model[0m[1m([0m                                                                       [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^                                                                       [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/modul[0m [2m                  [0m
[2;36m                      [0m         [32me.py"[0m, line [1;36m1736[0m, in _wrapped_call_impl                                                           [2m                  [0m
[2;36m                      [0m             return [1;35mself._call_impl[0m[1m([0m*args, **kwargs[1m)[0m                                                       [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                       [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/modul[0m [2m                  [0m
[2;36m                      [0m         [32me.py"[0m, line [1;36m1747[0m, in _call_impl                                                                   [2m                  [0m
[2;36m                      [0m             return [1;35mforward_call[0m[1m([0m*args, **kwargs[1m)[0m                                                          [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                          [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/tuners/tuners_uti[0m [2m                  [0m
[2;36m                      [0m         [32mls.py"[0m, line [1;36m107[0m, in forward                                                                      [2m                  [0m
[2;36m                      [0m             return [1;35mself.model.forward[0m[1m([0m*args, **kwargs[1m)[0m                                                    [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                    [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/models/ll[0m [2m                  [0m
[2;36m                      [0m         [32mama/modeling_llama.py"[0m, line [1;36m854[0m, in forward                                                      [2m                  [0m
[2;36m                      [0m             loss = [1;35mself.loss_function[0m[1m([0m[33mlogits[0m=[35mlogits[0m, [33mlabels[0m=[35mlabels[0m, [33mvocab_size[0m=[35mself[0m.config.vocab_size,    [2m                  [0m
[2;36m                      [0m         **kwargs[1m)[0m                                                                                         [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [2m                  [0m
[2;36m                      [0m         ^^^^^^^                                                                                           [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/loss/loss[0m [2m                  [0m
[2;36m                      [0m         [32m_utils.py"[0m, line [1;36m47[0m, in ForCausalLMLoss                                                           [2m                  [0m
[2;36m                      [0m             loss = [1;35mfixed_cross_entropy[0m[1m([0mshift_logits, shift_labels, num_items_in_batch, ignore_index,      [2m                  [0m
[2;36m                      [0m         **kwargs[1m)[0m                                                                                         [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [2m                  [0m
[2;36m                      [0m         ^^^^^                                                                                             [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/loss/loss[0m [2m                  [0m
[2;36m                      [0m         [32m_utils.py"[0m, line [1;36m26[0m, in fixed_cross_entropy                                                       [2m                  [0m
[2;36m                      [0m             loss = [1;35mnn.functional.cross_entropy[0m[1m([0msource, target, [33mignore_index[0m=[35mignore_index[0m,                 [2m                  [0m
[2;36m                      [0m         [33mreduction[0m=[35mreduction[0m[1m)[0m                                                                              [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [2m                  [0m
[2;36m                      [0m         ^^^^^                                                                                             [2m                  [0m
[2;36m                      [0m           File                                                                                            [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/functional.py[0m [2m                  [0m
[2;36m                      [0m         [32m"[0m, line [1;36m3479[0m, in cross_entropy                                                                    [2m                  [0m
[2;36m                      [0m             return [1;35mtorch._C._nn.cross_entropy_loss[0m[1m([0m                                                       [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                       [2m                  [0m
[2;36m                      [0m         torch.OutOfMemoryError: CUDA out of memory. Tried to allocate [1;36m768.00[0m MiB. GPU [1;36m0[0m has a total       [2m                  [0m
[2;36m                      [0m         capacity of [1;36m4.00[0m GiB of which [1;36m0[0m bytes is free. Including non-PyTorch memory, this process has     [2m                  [0m
[2;36m                      [0m         [1;36m17179869184.00[0m GiB memory in use. Of the allocated memory [1;36m4.73[0m GiB is allocated by PyTorch, and   [2m                  [0m
[2;36m                      [0m         [1;36m478.08[0m MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large    [2m                  [0m
[2;36m                      [0m         try setting [33mPYTORCH_CUDA_ALLOC_CONF[0m=[35mexpandable_segments[0m:[3;92mTrue[0m to avoid fragmentation.  See         [2m                  [0m
[2;36m                      [0m         documentation for Memory Management                                                               [2m                  [0m
[2;36m                      [0m         [1m([0m[4;94mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables[0m[4;94m)[0m                           [2m                  [0m
  0%|                                                                                                                     | 0/1724 [00:15<?, ?it/s]
