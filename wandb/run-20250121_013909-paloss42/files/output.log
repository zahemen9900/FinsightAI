  0%|                                                                                                                                                                    | 0/736 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 49%|█████████████████████████████████████████████████████████████████████████▎                                                                            | 360/736 [3:13:08<3:05:40, 29.63s/it]/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /HuggingFaceTB/SmolLM2-1.7B-Instruct/resolve/main/config.json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)')))"), '(Request ID: 00c30d06-9feb-4c55-b43d-76526beae38c)') - silently ignoring the lookup for the file config.json in HuggingFaceTB/SmolLM2-1.7B-Instruct.
{'loss': 21.1567, 'grad_norm': 8.451601028442383, 'learning_rate': 0.00019242636746143058, 'epoch': 0.07}
{'loss': 19.3574, 'grad_norm': 0.3734113574028015, 'learning_rate': 0.00017840112201963535, 'epoch': 0.14}
{'loss': 19.1589, 'grad_norm': 0.3567747175693512, 'learning_rate': 0.00016437587657784012, 'epoch': 0.2}
{'loss': 18.8569, 'grad_norm': 0.46158304810523987, 'learning_rate': 0.0001503506311360449, 'epoch': 0.27}
{'loss': 18.837, 'grad_norm': 0.44342491030693054, 'learning_rate': 0.00013632538569424967, 'epoch': 0.34}
{'loss': 18.761, 'grad_norm': 0.54407799243927, 'learning_rate': 0.00012230014025245442, 'epoch': 0.41}
{'loss': 18.7255, 'grad_norm': 0.4935443699359894, 'learning_rate': 0.0001082748948106592, 'epoch': 0.48}
  warnings.warn(                                                                                                                                                                                 
{'eval_loss': 2.315087080001831, 'eval_runtime': 1595.1203, 'eval_samples_per_second': 1.848, 'eval_steps_per_second': 0.924, 'epoch': 0.49}
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in HuggingFaceTB/SmolLM2-1.7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 720/736 [6:27:06<08:14, 30.88s/it]/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 18.5341, 'grad_norm': 0.4667201042175293, 'learning_rate': 9.424964936886396e-05, 'epoch': 0.54}
{'loss': 18.5512, 'grad_norm': 0.41543132066726685, 'learning_rate': 8.022440392706872e-05, 'epoch': 0.61}
{'loss': 18.5282, 'grad_norm': 0.4507623314857483, 'learning_rate': 6.61991584852735e-05, 'epoch': 0.68}
{'loss': 18.6958, 'grad_norm': 0.47092097997665405, 'learning_rate': 5.217391304347826e-05, 'epoch': 0.75}
{'loss': 18.5105, 'grad_norm': 0.5099563002586365, 'learning_rate': 3.814866760168303e-05, 'epoch': 0.81}
{'loss': 18.5354, 'grad_norm': 0.507062554359436, 'learning_rate': 2.41234221598878e-05, 'epoch': 0.88}
{'loss': 18.5138, 'grad_norm': 0.48269739747047424, 'learning_rate': 1.0098176718092566e-05, 'epoch': 0.95}
  return fn(*args, **kwargs)                                                                                                                                             
{'eval_loss': 2.302093505859375, 'eval_runtime': 1627.5462, 'eval_samples_per_second': 1.811, 'eval_steps_per_second': 0.906, 'epoch': 0.98}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 736/736 [6:35:13<00:00, 32.22s/it]
{'train_runtime': 23715.2936, 'train_samples_per_second': 0.497, 'train_steps_per_second': 0.031, 'train_loss': 18.878293742304262, 'epoch': 1.0}
***** train metrics *****
  epoch                    =     0.9993
  total_flos               = 79831967GF
  train_loss               =    18.8783
  train_runtime            = 6:35:15.29
  train_samples_per_second =      0.497
  train_steps_per_second   =      0.031
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1474/1474 [25:53<00:00,  1.05s/it]
***** eval metrics *****
  epoch                   =     0.9993
  eval_loss               =     2.3021
  eval_runtime            = 0:25:54.22
  eval_samples_per_second =      1.896
  eval_steps_per_second   =      0.948
