  0%|                                                                                                                                            | 0/533 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 47%|███████████████████████████████████████████████████████████                                                                   | 250/533 [3:30:38<2:46:16, 35.25s/it]/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 22.1661, 'grad_norm': 0.7618366479873657, 'learning_rate': 1.9963835472641704e-05, 'epoch': 0.06}
{'loss': 21.4181, 'grad_norm': 0.40615755319595337, 'learning_rate': 1.964469175054377e-05, 'epoch': 0.11}
{'loss': 20.7766, 'grad_norm': 0.42095598578453064, 'learning_rate': 1.900591881347924e-05, 'epoch': 0.17}
{'loss': 20.3282, 'grad_norm': 0.3545864224433899, 'learning_rate': 1.8068685870137906e-05, 'epoch': 0.22}
{'loss': 19.7904, 'grad_norm': 0.38928163051605225, 'learning_rate': 1.686405322163349e-05, 'epoch': 0.28}
{'loss': 19.7825, 'grad_norm': 0.6208500862121582, 'learning_rate': 1.5431942909823153e-05, 'epoch': 0.34}
{'loss': 19.5892, 'grad_norm': 0.7635646462440491, 'learning_rate': 1.381981568374366e-05, 'epoch': 0.39}
{'loss': 19.3874, 'grad_norm': 0.773838996887207, 'learning_rate': 1.208109813006316e-05, 'epoch': 0.45}
  return fn(*args, **kwargs)                                                                                                                                             
{'eval_loss': 2.4339215755462646, 'eval_runtime': 627.9022, 'eval_samples_per_second': 1.511, 'eval_steps_per_second': 0.756, 'epoch': 0.47}
 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 500/533 [9:11:49<1:41:50, 185.18s/it]/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 19.6952, 'grad_norm': 1.2390260696411133, 'learning_rate': 1.0273412093119003e-05, 'epoch': 0.51}
{'loss': 19.0072, 'grad_norm': 1.6547703742980957, 'learning_rate': 8.456665062319743e-06, 'epoch': 0.56}
{'loss': 19.2369, 'grad_norm': 6.845394611358643, 'learning_rate': 6.691064812290691e-06, 'epoch': 0.62}
{'loss': 19.5007, 'grad_norm': 1.1401506662368774, 'learning_rate': 5.035124091438735e-06, 'epoch': 0.67}
{'loss': 19.1978, 'grad_norm': 0.34565144777297974, 'learning_rate': 3.543721484411976e-06, 'epoch': 0.73}
{'loss': 18.8732, 'grad_norm': 0.3740154802799225, 'learning_rate': 2.266282712302672e-06, 'epoch': 0.79}
{'loss': 19.1007, 'grad_norm': 0.3215221166610718, 'learning_rate': 1.2451426430835733e-06, 'epoch': 0.84}
{'loss': 18.7498, 'grad_norm': 0.31001004576683044, 'learning_rate': 5.141422959533493e-07, 'epoch': 0.9}
  return fn(*args, **kwargs)                                                                                                                                             
{'eval_loss': 2.376391887664795, 'eval_runtime': 3300.4309, 'eval_samples_per_second': 0.288, 'eval_steps_per_second': 0.144, 'epoch': 0.94}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [10:54:03<00:00, 73.63s/it]
{'loss': 18.8808, 'grad_norm': 0.30633744597435, 'learning_rate': 9.75073354678846e-08, 'epoch': 0.96}
{'train_runtime': 39245.626, 'train_samples_per_second': 0.217, 'train_steps_per_second': 0.014, 'train_loss': 19.7052558483818, 'epoch': 1.0}
***** train metrics *****
  epoch                    =      0.9993
  total_flos               =  71401227GF
  train_loss               =     19.7053
  train_runtime            = 10:54:05.62
  train_samples_per_second =       0.217
  train_steps_per_second   =       0.014
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 475/475 [54:42<00:00,  6.91s/it]
***** eval metrics *****
  epoch                   =     0.9993
  eval_loss               =     2.3764
  eval_runtime            = 0:54:47.03
  eval_samples_per_second =      0.289
  eval_steps_per_second   =      0.145
