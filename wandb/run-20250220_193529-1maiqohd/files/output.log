  0%|                                                                                                                                  | 0/862 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[2;36m[2025-02-20 19:35:34][0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m02[0m-[1;36m20[0m [1;92m19:35:34[0m[1m][0m - ERROR - rich - Training failed: CUDA out of memory. Tried to allocate [1;36m768.00[0m MiB. GPU ]8;id=231148;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=471029;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#367\[2m367[0m]8;;\
[2;36m                      [0m         [1;36m0[0m has a total capacity of [1;36m4.00[0m GiB of which [1;36m0[0m bytes is free. Including non-PyTorch memory, this process has   [2m                  [0m
[2;36m                      [0m         [1;36m17179869184.00[0m GiB memory in use. Of the allocated memory [1;36m4.25[0m GiB is allocated by PyTorch, and [1;36m85.21[0m MiB is  [2m                  [0m
[2;36m                      [0m         reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting                  [2m                  [0m
[2;36m                      [0m         [33mPYTORCH_CUDA_ALLOC_CONF[0m=[35mexpandable_segments[0m:[3;92mTrue[0m to avoid fragmentation.  See documentation for Memory        [2m                  [0m
[2;36m                      [0m         Management  [1m([0m[4;94mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables[0m[4;94m)[0m                           [2m                  [0m
[2;36m                     [0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m02[0m-[1;36m20[0m [1;92m19:35:34[0m[1m][0m - ERROR - rich - Unhandled exception: CUDA out of memory. Tried to allocate [1;36m768.00[0m MiB. ]8;id=795667;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=844962;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#375\[2m375[0m]8;;\
[2;36m                      [0m         GPU [1;36m0[0m has a total capacity of [1;36m4.00[0m GiB of which [1;36m0[0m bytes is free. Including non-PyTorch memory, this process   [2m                  [0m
[2;36m                      [0m         has [1;36m17179869184.00[0m GiB memory in use. Of the allocated memory [1;36m4.25[0m GiB is allocated by PyTorch, and [1;36m85.21[0m MiB [2m                  [0m
[2;36m                      [0m         is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting               [2m                  [0m
[2;36m                      [0m         [33mPYTORCH_CUDA_ALLOC_CONF[0m=[35mexpandable_segments[0m:[3;92mTrue[0m to avoid fragmentation.  See documentation for Memory        [2m                  [0m
[2;36m                      [0m         Management  [1m([0m[4;94mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables[0m[4;94m)[0m                           [2m                  [0m
[2;36m                      [0m         Traceback [1m([0mmost recent call last[1m)[0m:                                                                            [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m372[0m, in [1m<[0m[1;95mmodule[0m[1m>[0m              [2m                  [0m
[2;36m                      [0m             [1;35mtrain[0m[1m([0m[1m)[0m                                                                                                   [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m347[0m, in train                 [2m                  [0m
[2;36m                      [0m             train_result = [1;35mtrainer.train[0m[1m([0m[1m)[0m                                                                            [2m                  [0m
[2;36m                      [0m                            ^^^^^^^^^^^^^^^                                                                            [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m2171[0m, in train                                                                                           [2m                  [0m
[2;36m                      [0m             return [1;35minner_training_loop[0m[1m([0m                                                                               [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^                                                                               [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m2531[0m, in _inner_training_loop                                                                            [2m                  [0m
[2;36m                      [0m             tr_loss_step = [1;35mself.training_step[0m[1m([0mmodel, inputs, num_items_in_batch[1m)[0m                                      [2m                  [0m
[2;36m                      [0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m3675[0m, in training_step                                                                                   [2m                  [0m
[2;36m                      [0m             loss = [1;35mself.compute_loss[0m[1m([0mmodel, inputs, [33mnum_items_in_batch[0m=[35mnum_items_in_batch[0m[1m)[0m                            [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                            [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m3731[0m, in compute_loss                                                                                    [2m                  [0m
[2;36m                      [0m             outputs = [1;35mmodel[0m[1m([0m**inputs[1m)[0m                                                                                 [2m                  [0m
[2;36m                      [0m                       ^^^^^^^^^^^^^^^                                                                                 [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/module.py"[0m, line  [2m                  [0m
[2;36m                      [0m         [1;36m1736[0m, in _wrapped_call_impl                                                                                   [2m                  [0m
[2;36m                      [0m             return [1;35mself._call_impl[0m[1m([0m*args, **kwargs[1m)[0m                                                                   [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                   [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/module.py"[0m, line  [2m                  [0m
[2;36m                      [0m         [1;36m1747[0m, in _call_impl                                                                                           [2m                  [0m
[2;36m                      [0m             return [1;35mforward_call[0m[1m([0m*args, **kwargs[1m)[0m                                                                      [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                      [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/utils/operations.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m819[0m, in forward                                                                                          [2m                  [0m
[2;36m                      [0m             return [1;35mmodel_forward[0m[1m([0m*args, **kwargs[1m)[0m                                                                     [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                     [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/utils/operations.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m807[0m, in __call__                                                                                         [2m                  [0m
[2;36m                      [0m             return [1;35mconvert_to_fp32[0m[1m([0m[1;35mself.model_forward[0m[1m([0m*args, **kwargs[1m)[0m[1m)[0m                                               [2m                  [0m
[2;36m                      [0m                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/amp/autocast_mode.py"[0m, line  [2m                  [0m
[2;36m                      [0m         [1;36m44[0m, in decorate_autocast                                                                                      [2m                  [0m
[2;36m                      [0m             return [1;35mfunc[0m[1m([0m*args, **kwargs[1m)[0m                                                                              [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^                                                                              [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/peft_model.py"[0m, line   [2m                  [0m
[2;36m                      [0m         [1;36m1003[0m, in forward                                                                                              [2m                  [0m
[2;36m                      [0m             return [1;35mself.base_model[0m[1m([0m                                                                                   [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^                                                                                   [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/module.py"[0m, line  [2m                  [0m
[2;36m                      [0m         [1;36m1736[0m, in _wrapped_call_impl                                                                                   [2m                  [0m
[2;36m                      [0m             return [1;35mself._call_impl[0m[1m([0m*args, **kwargs[1m)[0m                                                                   [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                   [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/module.py"[0m, line  [2m                  [0m
[2;36m                      [0m         [1;36m1747[0m, in _call_impl                                                                                           [2m                  [0m
[2;36m                      [0m             return [1;35mforward_call[0m[1m([0m*args, **kwargs[1m)[0m                                                                      [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                      [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/tuners/tuners_utils.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m107[0m, in forward                                                                                               [2m                  [0m
[2;36m                      [0m             return [1;35mself.model.forward[0m[1m([0m*args, **kwargs[1m)[0m                                                                [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/models/llama/modeling[0m [2m                  [0m
[2;36m                      [0m         [32m_llama.py"[0m, line [1;36m854[0m, in forward                                                                              [2m                  [0m
[2;36m                      [0m             loss = [1;35mself.loss_function[0m[1m([0m[33mlogits[0m=[35mlogits[0m, [33mlabels[0m=[35mlabels[0m, [33mvocab_size[0m=[35mself[0m.config.vocab_size, **kwargs[1m)[0m      [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^      [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/loss/loss_utils.py"[0m,  [2m                  [0m
[2;36m                      [0m         line [1;36m47[0m, in ForCausalLMLoss                                                                                   [2m                  [0m
[2;36m                      [0m             loss = [1;35mfixed_cross_entropy[0m[1m([0mshift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs[1m)[0m        [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^        [2m                  [0m
[2;36m                      [0m           File                                                                                                        [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/loss/loss_utils.py"[0m,  [2m                  [0m
[2;36m                      [0m         line [1;36m26[0m, in fixed_cross_entropy                                                                               [2m                  [0m
[2;36m                      [0m             loss = [1;35mnn.functional.cross_entropy[0m[1m([0msource, target, [33mignore_index[0m=[35mignore_index[0m, [33mreduction[0m=[35mreduction[0m[1m)[0m        [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^        [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/functional.py"[0m,    [2m                  [0m
[2;36m                      [0m         line [1;36m3479[0m, in cross_entropy                                                                                   [2m                  [0m
[2;36m                      [0m             return [1;35mtorch._C._nn.cross_entropy_loss[0m[1m([0m                                                                   [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                   [2m                  [0m
[2;36m                      [0m         torch.OutOfMemoryError: CUDA out of memory. Tried to allocate [1;36m768.00[0m MiB. GPU [1;36m0[0m has a total capacity of [1;36m4.00[0m  [2m                  [0m
[2;36m                      [0m         GiB of which [1;36m0[0m bytes is free. Including non-PyTorch memory, this process has [1;36m17179869184.00[0m GiB memory in     [2m                  [0m
[2;36m                      [0m         use. Of the allocated memory [1;36m4.25[0m GiB is allocated by PyTorch, and [1;36m85.21[0m MiB is reserved by PyTorch but       [2m                  [0m
[2;36m                      [0m         unallocated. If reserved but unallocated memory is large try setting                                          [2m                  [0m
[2;36m                      [0m         [33mPYTORCH_CUDA_ALLOC_CONF[0m=[35mexpandable_segments[0m:[3;92mTrue[0m to avoid fragmentation.  See documentation for Memory        [2m                  [0m
[2;36m                      [0m         Management  [1m([0m[4;94mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables[0m[4;94m)[0m                           [2m                  [0m
  0%|                                                                                                                                  | 0/862 [00:03<?, ?it/s]
