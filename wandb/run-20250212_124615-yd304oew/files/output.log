  0%|                                                                                                                                           | 0/1872 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                              | 700/1872 [7:36:47<8:35:22, 26.38s/it]
{'loss': 2.7548, 'grad_norm': 0.05771232023835182, 'learning_rate': 5.263157894736842e-05, 'epoch': 0.03}
{'loss': 2.5614, 'grad_norm': 0.07905380427837372, 'learning_rate': 5.857851239669422e-05, 'epoch': 0.05}
{'loss': 2.4316, 'grad_norm': 0.0692196935415268, 'learning_rate': 5.692561983471075e-05, 'epoch': 0.08}
{'loss': 2.3269, 'grad_norm': 0.06283388286828995, 'learning_rate': 5.527272727272728e-05, 'epoch': 0.11}
{'loss': 2.3016, 'grad_norm': 0.07172577828168869, 'learning_rate': 5.3619834710743805e-05, 'epoch': 0.13}
{'loss': 2.2624, 'grad_norm': 0.06742653250694275, 'learning_rate': 5.196694214876034e-05, 'epoch': 0.16}
{'loss': 2.2614, 'grad_norm': 0.06769738346338272, 'learning_rate': 5.031404958677686e-05, 'epoch': 0.19}
{'loss': 2.2236, 'grad_norm': 0.13709421455860138, 'learning_rate': 4.866115702479339e-05, 'epoch': 0.21}
{'loss': 2.1866, 'grad_norm': 0.12073744088411331, 'learning_rate': 4.7008264462809915e-05, 'epoch': 0.24}
{'loss': 2.2255, 'grad_norm': 0.09508823603391647, 'learning_rate': 4.535537190082645e-05, 'epoch': 0.27}
{'loss': 2.2428, 'grad_norm': 0.09471374750137329, 'learning_rate': 4.3702479338842974e-05, 'epoch': 0.29}
{'loss': 2.2205, 'grad_norm': 0.107921302318573, 'learning_rate': 4.2049586776859506e-05, 'epoch': 0.32}
{'loss': 2.1828, 'grad_norm': 0.07777588814496994, 'learning_rate': 4.039669421487603e-05, 'epoch': 0.35}
{'loss': 2.1599, 'grad_norm': 0.10147459805011749, 'learning_rate': 3.8743801652892565e-05, 'epoch': 0.37}
  0%|                                                                                                                                            | 0/832 [00:00<?, ?it/s]
[2;36m[2025-02-12 20:23:08][0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m02[0m-[1;36m12[0m [1;92m20:23:08[0m[1m][0m - ERROR - rich - Training failed: CUDA error: out of memory                                       ]8;id=231148;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=471029;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#366\[2m366[0m]8;;\
[2;36m                      [0m         CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. [2m                  [0m
[2;36m                      [0m         For debugging consider passing [33mCUDA_LAUNCH_BLOCKING[0m=[1;36m1[0m                                                                   [2m                  [0m
[2;36m                      [0m         Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                     [2m                  [0m
[2;36m                      [0m                                                                                                                                 [2m                  [0m
[2;36m                     [0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m02[0m-[1;36m12[0m [1;92m20:23:08[0m[1m][0m - ERROR - rich - Unhandled exception: CUDA error: out of memory                                   ]8;id=795667;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=844962;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#374\[2m374[0m]8;;\
[2;36m                      [0m         CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. [2m                  [0m
[2;36m                      [0m         For debugging consider passing [33mCUDA_LAUNCH_BLOCKING[0m=[1;36m1[0m                                                                   [2m                  [0m
[2;36m                      [0m         Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                     [2m                  [0m
[2;36m                      [0m         Traceback [1m([0mmost recent call last[1m)[0m:                                                                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m371[0m, in [1m<[0m[1;95mmodule[0m[39m>[0m                        [2m                  [0m
[2;36m                      [0m         [39m    [0m[1;35mtrain[0m[1;39m([0m[1;39m)[0m                                                                                                             [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m[39m, line [0m[1;36m346[0m[39m, in train[0m                           [2m                  [0m
[2;36m                      [0m         [39m    train_result = [0m[1;35mtrainer.train[0m[1;39m([0m[1;39m)[0m                                                                                      [2m                  [0m
[2;36m                      [0m         [39m                   ^^^^^^^^^^^^^^^[0m                                                                                      [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m[39m, line [0m[1;36m2171[0m[39m, [0m [2m                  [0m
[2;36m                      [0m         [39min train[0m                                                                                                                [2m                  [0m
[2;36m                      [0m         [39m    return [0m[1;35minner_training_loop[0m[1;39m([0m                                                                                         [2m                  [0m
[2;36m                      [0m         [39m           ^^^^^^^^^^^^^^^^^^^^[0m                                                                                         [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m[39m, line [0m[1;36m2598[0m[39m, [0m [2m                  [0m
[2;36m                      [0m         [39min _inner_training_loop[0m                                                                                                 [2m                  [0m
[2;36m                      [0m         [39m    [0m[1;35mself._maybe_log_save_evaluate[0m[1;39m([0m                                                                                      [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m[39m, line [0m[1;36m3071[0m[39m, [0m [2m                  [0m
[2;36m                      [0m         [39min _maybe_log_save_evaluate[0m                                                                                             [2m                  [0m
[2;36m                      [0m         [39m    metrics = [0m[1;35mself._evaluate[0m[1;39m([0m[39mtrial, ignore_keys_for_eval[0m[1;39m)[0m                                                               [2m                  [0m
[2;36m                      [0m         [39m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m                                                               [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m[39m, line [0m[1;36m3025[0m[39m, [0m [2m                  [0m
[2;36m                      [0m         [39min _evaluate[0m                                                                                                            [2m                  [0m
[2;36m                      [0m         [39m    metrics = [0m[1;35mself.evaluate[0m[1;39m([0m[33mignore_keys[0m[39m=[0m[35mignore_keys_for_eval[0m[1;39m)[0m                                                           [2m                  [0m
[2;36m                      [0m         [39m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m                                                           [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m[39m, line [0m[1;36m4073[0m[39m, [0m [2m                  [0m
[2;36m                      [0m         [39min evaluate[0m                                                                                                             [2m                  [0m
[2;36m                      [0m         [39m    output = [0m[1;35meval_loop[0m[1;39m([0m                                                                                                 [2m                  [0m
[2;36m                      [0m         [39m             ^^^^^^^^^^[0m                                                                                                 [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m[39m, line [0m[1;36m4257[0m[39m, [0m [2m                  [0m
[2;36m                      [0m         [39min evaluation_loop[0m                                                                                                      [2m                  [0m
[2;36m                      [0m         [39m    for step, inputs in [0m[1;35menumerate[0m[1;39m([0m[39mdataloader[0m[1;39m)[0m[39m:[0m                                                                          [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/data_loader.py"[0m[39m, line [0m[1;36m572[0m[39m,[0m [2m                  [0m
[2;36m                      [0m         [39min __iter__[0m                                                                                                             [2m                  [0m
[2;36m                      [0m         [39m    current_batch = [0m[1;35msend_to_device[0m[1;39m([0m[39mcurrent_batch, self.device, [0m[33mnon_blocking[0m[39m=[0m[35mself[0m[39m._non_blocking[0m[1;39m)[0m                         [2m                  [0m
[2;36m                      [0m         [39m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m                         [2m                  [0m
[2;36m                      [0m         [39m  File [0m[32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/utils/operations.py"[0m[39m, line[0m [2m                  [0m
[2;36m                      [0m         [1;36m155[0m[39m, in send_to_device[0m                                                                                                  [2m                  [0m
[2;36m                      [0m         [39m    return [0m[1;35mtensor.to[0m[1;39m([0m[39mdevice, [0m[33mnon_blocking[0m[39m=[0m[35mnon_blocking[0m[1;39m)[0m                                                                 [2m                  [0m
[2;36m                      [0m         [39m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m                                                                 [2m                  [0m
[2;36m                      [0m         [39m  File [0m                                                                                                                 [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/tokenization_utils_base.py"[0m[39m, [0m   [2m                  [0m
[2;36m                      [0m         [39mline [0m[1;36m820[0m[39m, in to[0m                                                                                                         [2m                  [0m
[2;36m                      [0m         [39m    self.data = [0m[1;39m{[0m                                                                                                       [2m                  [0m
[2;36m                      [0m         [39m                ^[0m                                                                                                       [2m                  [0m
[2;36m                      [0m         [39m  File [0m                                                                                                                 [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/tokenization_utils_base.py"[0m[39m, [0m   [2m                  [0m
[2;36m                      [0m         [39mline [0m[1;36m821[0m[39m, in <dictcomp[0m[1m>[0m                                                                                                 [2m                  [0m
[2;36m                      [0m             k: [1;35mv.to[0m[1m([0m[33mdevice[0m=[35mdevice[0m, [33mnon_blocking[0m=[35mnon_blocking[0m[1m)[0m if [1;35misinstance[0m[1m([0mv, torch.Tensor[1m)[0m else v                             [2m                  [0m
[2;36m                      [0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                   [2m                  [0m
[2;36m                      [0m         RuntimeError: CUDA error: out of memory                                                                                 [2m                  [0m
[2;36m                      [0m         CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. [2m                  [0m
[2;36m                      [0m         For debugging consider passing [33mCUDA_LAUNCH_BLOCKING[0m=[1;36m1[0m                                                                   [2m                  [0m
[2;36m                      [0m         Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                     [2m                  [0m
[2;36m                      [0m                                                                                                                                 [2m                  [0m

