  0%|                                                                                                                                            | 0/400 [00:00<?, ?it/s]
[2;36m[2025-01-27 11:25:24][0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m01[0m-[1;36m27[0m [1;92m11:25:24[0m[1m][0m - ERROR - rich - Training failed: Asking to pad but the tokenizer does not have a padding token.  ]8;id=231148;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=471029;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#282\[2m282[0m]8;;\
[2;36m                      [0m         Please select a token to use as `pad_token` `[1m([0mtokenizer.pad_token = tokenizer.eos_token e.g.[1m)[0m` or add a new pad token   [2m                  [0m
[2;36m                      [0m         via `[1;35mtokenizer.add_special_tokens[0m[1m([0m[1m{[0m[32m'pad_token'[0m: [32m'[0m[32m[[0m[32mPAD[0m[32m][0m[32m'[0m[1m}[0m[1m)[0m`.                                                             [2m                  [0m
[2;36m                     [0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m01[0m-[1;36m27[0m [1;92m11:25:24[0m[1m][0m - ERROR - rich - Unhandled exception: Asking to pad but the tokenizer does not have a padding     ]8;id=795667;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=844962;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#290\[2m290[0m]8;;\
[2;36m                      [0m         token. Please select a token to use as `pad_token` `[1m([0mtokenizer.pad_token = tokenizer.eos_token e.g.[1m)[0m` or add a new pad  [2m                  [0m
[2;36m                      [0m         token via `[1;35mtokenizer.add_special_tokens[0m[1m([0m[1m{[0m[32m'pad_token'[0m: [32m'[0m[32m[[0m[32mPAD[0m[32m][0m[32m'[0m[1m}[0m[1m)[0m`.                                                       [2m                  [0m
[2;36m                      [0m         Traceback [1m([0mmost recent call last[1m)[0m:                                                                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m287[0m, in [1m<[0m[1;95mmodule[0m[1m>[0m                        [2m                  [0m
[2;36m                      [0m             [1;35mtrain[0m[1m([0m[1m)[0m                                                                                                             [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m262[0m, in train                           [2m                  [0m
[2;36m                      [0m             train_result = [1;35mtrainer.train[0m[1m([0m[1m)[0m                                                                                      [2m                  [0m
[2;36m                      [0m                            ^^^^^^^^^^^^^^^                                                                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [1;36m2171[0m,  [2m                  [0m
[2;36m                      [0m         in train                                                                                                                [2m                  [0m
[2;36m                      [0m             return [1;35minner_training_loop[0m[1m([0m                                                                                         [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^                                                                                         [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [1;36m2480[0m,  [2m                  [0m
[2;36m                      [0m         in _inner_training_loop                                                                                                 [2m                  [0m
[2;36m                      [0m             batch_samples, num_items_in_batch = [1;35mself.get_batch_samples[0m[1m([0mepoch_iterator, num_batches[1m)[0m                             [2m                  [0m
[2;36m                      [0m                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                             [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [1;36m5156[0m,  [2m                  [0m
[2;36m                      [0m         in get_batch_samples                                                                                                    [2m                  [0m
[2;36m                      [0m             batch_samples += [1m[[0m[1;35mnext[0m[1m([0mepoch_iterator[1m)[0m[1m][0m                                                                             [2m                  [0m
[2;36m                      [0m                               ^^^^^^^^^^^^^^^^^^^^                                                                              [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/data_loader.py"[0m, line [1;36m552[0m, [2m                  [0m
[2;36m                      [0m         in __iter__                                                                                                             [2m                  [0m
[2;36m                      [0m             current_batch = [1;35mnext[0m[1m([0mdataloader_iter[1m)[0m                                                                               [2m                  [0m
[2;36m                      [0m                             ^^^^^^^^^^^^^^^^^^^^^                                                                               [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/utils/data/dataloader.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m701[0m, in __next__                                                                                                        [2m                  [0m
[2;36m                      [0m             data = [1;35mself._next_data[0m[1m([0m[1m)[0m                                                                                            [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^                                                                                            [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/utils/data/dataloader.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m757[0m, in _next_data                                                                                                      [2m                  [0m
[2;36m                      [0m             data = [1;35mself._dataset_fetcher.fetch[0m[1m([0mindex[1m)[0m  # may raise StopIteration                                                [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                           [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py"[0m,    [2m                  [0m
[2;36m                      [0m         line [1;36m55[0m, in fetch                                                                                                       [2m                  [0m
[2;36m                      [0m             return [1;35mself.collate_fn[0m[1m([0mdata[1m)[0m                                                                                        [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^                                                                                        [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/data/data_collator.py"[0m,  [2m                  [0m
[2;36m                      [0m         line [1;36m45[0m, in __call__                                                                                                    [2m                  [0m
[2;36m                      [0m             return [1;35mself.torch_call[0m[1m([0mfeatures[1m)[0m                                                                                    [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                    [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/data/data_collator.py"[0m,  [2m                  [0m
[2;36m                      [0m         line [1;36m810[0m, in torch_call                                                                                                 [2m                  [0m
[2;36m                      [0m             batch = [1;35mpad_without_fast_tokenizer_warning[0m[1m([0m                                                                         [2m                  [0m
[2;36m                      [0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                         [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/data/data_collator.py"[0m,  [2m                  [0m
[2;36m                      [0m         line [1;36m66[0m, in pad_without_fast_tokenizer_warning                                                                          [2m                  [0m
[2;36m                      [0m             padded = [1;35mtokenizer.pad[0m[1m([0m*pad_args, **pad_kwargs[1m)[0m                                                                     [2m                  [0m
[2;36m                      [0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                     [2m                  [0m
[2;36m                      [0m           File                                                                                                                  [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/tokenization_utils_base.py"[0m,    [2m                  [0m
[2;36m                      [0m         line [1;36m3346[0m, in pad                                                                                                       [2m                  [0m
[2;36m                      [0m             padding_strategy, _, max_length, _ = [1;35mself._get_padding_truncation_strategies[0m[1m([0m                                       [2m                  [0m
[2;36m                      [0m                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                       [2m                  [0m
[2;36m                      [0m           File                                                                                                                  [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/tokenization_utils_base.py"[0m,    [2m                  [0m
[2;36m                      [0m         line [1;36m2770[0m, in _get_padding_truncation_strategies                                                                        [2m                  [0m
[2;36m                      [0m             raise [1;35mValueError[0m[1m([0m                                                                                                   [2m                  [0m
[2;36m                      [0m         ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token`  [2m                  [0m
[2;36m                      [0m         `[1m([0mtokenizer.pad_token = tokenizer.eos_token e.g.[1m)[0m` or add a new pad token via                                           [2m                  [0m
[2;36m                      [0m         `[1;35mtokenizer.add_special_tokens[0m[1m([0m[1m{[0m[32m'pad_token'[0m: [32m'[0m[32m[[0m[32mPAD[0m[32m][0m[32m'[0m[1m}[0m[1m)[0m`.                                                                 [2m                  [0m
