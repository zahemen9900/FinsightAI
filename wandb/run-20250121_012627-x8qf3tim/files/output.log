  0%|                                                                                                                                                                    | 0/736 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
[2;36m[2025-01-21 01:26:29][0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m01[0m-[1;36m21[0m [1;92m01:26:29[0m[1m][0m - ERROR - rich - Training failed: Expected to mark a variable ready only once. This error is caused by one of the         ]8;id=231148;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=471029;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#214\[2m214[0m]8;;\
[2;36m                      [0m         following reasons: [1;36m1[0m[1m)[0m Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across         [2m                  [0m
[2;36m                      [0m         multiple concurrent forward-backward passes. or try to use [1;35m_set_static_graph[0m[1m([0m[1m)[0m as a workaround if this module graph does not change during      [2m                  [0m
[2;36m                      [0m         training loop.[1;36m2[0m[1m)[0m Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the   [2m                  [0m
[2;36m                      [0m         same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and     [2m                  [0m
[2;36m                      [0m         hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use [1;35m_set_static_graph[0m[1m([0m[1m)[0m as a      [2m                  [0m
[2;36m                      [0m         workaround if your module graph does not change over iterations.                                                                                [2m                  [0m
[2;36m                      [0m         Parameter at index [1;36m335[0m with name base_model.model.model.layers.[1;36m23.[0mmlp.down_proj.lora_B.default.weight has been marked as ready twice. This      [2m                  [0m
[2;36m                      [0m         means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.                                      [2m                  [0m
[2;36m                     [0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m01[0m-[1;36m21[0m [1;92m01:26:29[0m[1m][0m - ERROR - rich - Unhandled exception: Expected to mark a variable ready only once. This error is caused by one of the     ]8;id=795667;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=844962;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#222\[2m222[0m]8;;\
[2;36m                      [0m         following reasons: [1;36m1[0m[1m)[0m Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across         [2m                  [0m
[2;36m                      [0m         multiple concurrent forward-backward passes. or try to use [1;35m_set_static_graph[0m[1m([0m[1m)[0m as a workaround if this module graph does not change during      [2m                  [0m
[2;36m                      [0m         training loop.[1;36m2[0m[1m)[0m Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the   [2m                  [0m
[2;36m                      [0m         same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and     [2m                  [0m
[2;36m                      [0m         hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use [1;35m_set_static_graph[0m[1m([0m[1m)[0m as a      [2m                  [0m
[2;36m                      [0m         workaround if your module graph does not change over iterations.                                                                                [2m                  [0m
[2;36m                      [0m         Parameter at index [1;36m335[0m with name base_model.model.model.layers.[1;36m23.[0mmlp.down_proj.lora_B.default.weight has been marked as ready twice. This      [2m                  [0m
[2;36m                      [0m         means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.                                      [2m                  [0m
[2;36m                      [0m         Traceback [1m([0mmost recent call last[1m)[0m:                                                                                                              [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m219[0m, in [1m<[0m[1;95mmodule[0m[1m>[0m                                                [2m                  [0m
[2;36m                      [0m             [1;35mtrain[0m[1m([0m[1m)[0m                                                                                                                                     [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m194[0m, in train                                                   [2m                  [0m
[2;36m                      [0m             train_result = [1;35mtrainer.train[0m[1m([0m[1m)[0m                                                                                                              [2m                  [0m
[2;36m                      [0m                            ^^^^^^^^^^^^^^^                                                                                                              [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [1;36m2171[0m, in train                 [2m                  [0m
[2;36m                      [0m             return [1;35minner_training_loop[0m[1m([0m                                                                                                                 [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^                                                                                                                 [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [1;36m2531[0m, in _inner_training_loop  [2m                  [0m
[2;36m                      [0m             tr_loss_step = [1;35mself.training_step[0m[1m([0mmodel, inputs, num_items_in_batch[1m)[0m                                                                        [2m                  [0m
[2;36m                      [0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                        [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [1;36m3715[0m, in training_step         [2m                  [0m
[2;36m                      [0m             [1;35mself.accelerator.backward[0m[1m([0mloss, **kwargs[1m)[0m                                                                                                   [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/accelerator.py"[0m, line [1;36m2248[0m, in backward            [2m                  [0m
[2;36m                      [0m             [1;35mloss.backward[0m[1m([0m**kwargs[1m)[0m                                                                                                                     [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_tensor.py"[0m, line [1;36m581[0m, in backward                      [2m                  [0m
[2;36m                      [0m             [1;35mtorch.autograd.backward[0m[1m([0m                                                                                                                    [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/autograd/__init__.py"[0m, line [1;36m347[0m, in backward            [2m                  [0m
[2;36m                      [0m             [1;35m_engine_run_backward[0m[1m([0m                                                                                                                       [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/autograd/graph.py"[0m, line [1;36m825[0m, in _engine_run_backward   [2m                  [0m
[2;36m                      [0m             return [1;35mVariable._execution_engine.run_backward[0m[1m([0m  # Calls into the C++ engine to run the backward pass                                       [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                       [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/autograd/function.py"[0m, line [1;36m307[0m, in apply               [2m                  [0m
[2;36m                      [0m             return [1;35muser_fn[0m[1m([0mself, *args[1m)[0m                                                                                                                 [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^                                                                                                                 [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/utils/checkpoint.py"[0m, line [1;36m321[0m, in backward             [2m                  [0m
[2;36m                      [0m             [1;35mtorch.autograd.backward[0m[1m([0moutputs_with_grad, args_with_grad[1m)[0m                                                                                  [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/autograd/__init__.py"[0m, line [1;36m347[0m, in backward            [2m                  [0m
[2;36m                      [0m             [1;35m_engine_run_backward[0m[1m([0m                                                                                                                       [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/autograd/graph.py"[0m, line [1;36m825[0m, in _engine_run_backward   [2m                  [0m
[2;36m                      [0m             return [1;35mVariable._execution_engine.run_backward[0m[1m([0m  # Calls into the C++ engine to run the backward pass                                       [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                       [2m                  [0m
[2;36m                      [0m         RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: [1;36m1[0m[1m)[0m Use of a module parameter   [2m                  [0m
[2;36m                      [0m         outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to  [2m                  [0m
[2;36m                      [0m         use [1;35m_set_static_graph[0m[1m([0m[1m)[0m as a workaround if this module graph does not change during training loop.[1;36m2[0m[1m)[0m Reused parameters in multiple reentrant    [2m                  [0m
[2;36m                      [0m         backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set   [2m                  [0m
[2;36m                      [0m         of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not  [2m                  [0m
[2;36m                      [0m         support such use cases in default. You can try to use [1;35m_set_static_graph[0m[1m([0m[1m)[0m as a workaround if your module graph does not change over iterations. [2m                  [0m
[2;36m                      [0m         Parameter at index [1;36m335[0m with name base_model.model.model.layers.[1;36m23.[0mmlp.down_proj.lora_B.default.weight has been marked as ready twice. This      [2m                  [0m
[2;36m                      [0m         means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.                                      [2m                  [0m
  0%|                                                                                                                                                                    | 0/736 [00:00<?, ?it/s]
