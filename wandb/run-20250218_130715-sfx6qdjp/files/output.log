  0%|                                                                                                                                            | 0/362 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|██▏                                                                                                                               | 6/362 [02:03<2:06:49, 21.37s/it]Traceback (most recent call last):
  File "/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py", line 371, in <module>
    train()
  File "/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py", line 346, in train
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py", line 2536, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
