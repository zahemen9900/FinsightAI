  0%|                                                                                                                                    | 0/1158 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[2;36m[2025-02-06 11:38:29][0m[2;36m [0m[1;31mERROR   [0m [1m[[0m[1;36m2025[0m-[1;36m02[0m-[1;36m06[0m [1;92m11:38:29[0m[1m][0m - ERROR - rich - Unhandled exception: CUDA out of memory. Tried to allocate [1;36m768.00[0m MiB.    ]8;id=231148;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py\[2mtrain_qlora.py[0m]8;;\[2m:[0m]8;id=471029;file:///home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py#381\[2m381[0m]8;;\
[2;36m                      [0m         GPU [1;36m0[0m has a total capacity of [1;36m4.00[0m GiB of which [1;36m0[0m bytes is free. Including non-PyTorch memory, this process has  [2m                  [0m
[2;36m                      [0m         [1;36m17179869184.00[0m GiB memory in use. Of the allocated memory [1;36m4.45[0m GiB is allocated by PyTorch, and [1;36m86.21[0m MiB is     [2m                  [0m
[2;36m                      [0m         reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting                     [2m                  [0m
[2;36m                      [0m         [33mPYTORCH_CUDA_ALLOC_CONF[0m=[35mexpandable_segments[0m:[3;92mTrue[0m to avoid fragmentation.  See documentation for Memory           [2m                  [0m
[2;36m                      [0m         Management  [1m([0m[4;94mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables[0m[4;94m)[0m                              [2m                  [0m
[2;36m                      [0m         Traceback [1m([0mmost recent call last[1m)[0m:                                                                               [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m378[0m, in [1m<[0m[1;95mmodule[0m[1m>[0m                 [2m                  [0m
[2;36m                      [0m             [1;35mtrain[0m[1m([0m[1m)[0m                                                                                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py"[0m, line [1;36m348[0m, in train                    [2m                  [0m
[2;36m                      [0m             [1;35mtrainer.train[0m[1m([0m[1m)[0m                                                                                              [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m2171[0m, in train                                                                                                   [2m                  [0m
[2;36m                      [0m             return [1;35minner_training_loop[0m[1m([0m                                                                                  [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^                                                                                  [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m2531[0m, in _inner_training_loop                                                                                    [2m                  [0m
[2;36m                      [0m             tr_loss_step = [1;35mself.training_step[0m[1m([0mmodel, inputs, num_items_in_batch[1m)[0m                                         [2m                  [0m
[2;36m                      [0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                         [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m3676[0m, in training_step                                                                                           [2m                  [0m
[2;36m                      [0m             loss = [1;35mself.compute_loss[0m[1m([0mmodel, inputs[1m)[0m                                                                      [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m3734[0m, in compute_loss                                                                                            [2m                  [0m
[2;36m                      [0m             outputs = [1;35mmodel[0m[1m([0m**inputs[1m)[0m                                                                                    [2m                  [0m
[2;36m                      [0m                       ^^^^^^^^^^^^^^^                                                                                    [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/module.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m1736[0m, in _wrapped_call_impl                                                                                 [2m                  [0m
[2;36m                      [0m             return [1;35mself._call_impl[0m[1m([0m*args, **kwargs[1m)[0m                                                                      [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/module.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m1747[0m, in _call_impl                                                                                         [2m                  [0m
[2;36m                      [0m             return [1;35mforward_call[0m[1m([0m*args, **kwargs[1m)[0m                                                                         [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                         [2m                  [0m
[2;36m                      [0m           File                                                                                                           [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/utils/operations.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m823[0m, in forward                                                                                                  [2m                  [0m
[2;36m                      [0m             return [1;35mmodel_forward[0m[1m([0m*args, **kwargs[1m)[0m                                                                        [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                        [2m                  [0m
[2;36m                      [0m           File                                                                                                           [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/accelerate/utils/operations.py"[0m, line [2m                  [0m
[2;36m                      [0m         [1;36m811[0m, in __call__                                                                                                 [2m                  [0m
[2;36m                      [0m             return [1;35mconvert_to_fp32[0m[1m([0m[1;35mself.model_forward[0m[1m([0m*args, **kwargs[1m)[0m[1m)[0m                                                  [2m                  [0m
[2;36m                      [0m                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                   [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/amp/autocast_mode.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m44[0m, in decorate_autocast                                                                                    [2m                  [0m
[2;36m                      [0m             return [1;35mfunc[0m[1m([0m*args, **kwargs[1m)[0m                                                                                 [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^                                                                                 [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/peft_model.py"[0m, line      [2m                  [0m
[2;36m                      [0m         [1;36m1719[0m, in forward                                                                                                 [2m                  [0m
[2;36m                      [0m             return [1;35mself.base_model[0m[1m([0m                                                                                      [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^                                                                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/module.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m1736[0m, in _wrapped_call_impl                                                                                 [2m                  [0m
[2;36m                      [0m             return [1;35mself._call_impl[0m[1m([0m*args, **kwargs[1m)[0m                                                                      [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                      [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/modules/module.py"[0m,   [2m                  [0m
[2;36m                      [0m         line [1;36m1747[0m, in _call_impl                                                                                         [2m                  [0m
[2;36m                      [0m             return [1;35mforward_call[0m[1m([0m*args, **kwargs[1m)[0m                                                                         [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                         [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/peft/tuners/tuners_utils.py"[0m,  [2m                  [0m
[2;36m                      [0m         line [1;36m197[0m, in forward                                                                                             [2m                  [0m
[2;36m                      [0m             return [1;35mself.model.forward[0m[1m([0m*args, **kwargs[1m)[0m                                                                   [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                   [2m                  [0m
[2;36m                      [0m           File                                                                                                           [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/models/llama/modeling_ll[0m [2m                  [0m
[2;36m                      [0m         [32mama.py"[0m, line [1;36m851[0m, in forward                                                                                    [2m                  [0m
[2;36m                      [0m             loss = [1;35mself.loss_function[0m[1m([0m[33mlogits[0m=[35mlogits[0m, [33mlabels[0m=[35mlabels[0m, [33mvocab_size[0m=[35mself[0m.config.vocab_size, **kwargs[1m)[0m         [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         [2m                  [0m
[2;36m                      [0m           File                                                                                                           [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/loss/loss_utils.py"[0m,     [2m                  [0m
[2;36m                      [0m         line [1;36m47[0m, in ForCausalLMLoss                                                                                      [2m                  [0m
[2;36m                      [0m             loss = [1;35mfixed_cross_entropy[0m[1m([0mshift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs[1m)[0m           [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^           [2m                  [0m
[2;36m                      [0m           File                                                                                                           [2m                  [0m
[2;36m                      [0m         [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/loss/loss_utils.py"[0m,     [2m                  [0m
[2;36m                      [0m         line [1;36m26[0m, in fixed_cross_entropy                                                                                  [2m                  [0m
[2;36m                      [0m             loss = [1;35mnn.functional.cross_entropy[0m[1m([0msource, target, [33mignore_index[0m=[35mignore_index[0m, [33mreduction[0m=[35mreduction[0m[1m)[0m           [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^           [2m                  [0m
[2;36m                      [0m           File [32m"/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/nn/functional.py"[0m, line  [2m                  [0m
[2;36m                      [0m         [1;36m3479[0m, in cross_entropy                                                                                           [2m                  [0m
[2;36m                      [0m             return [1;35mtorch._C._nn.cross_entropy_loss[0m[1m([0m                                                                      [2m                  [0m
[2;36m                      [0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                      [2m                  [0m
[2;36m                      [0m         torch.OutOfMemoryError: CUDA out of memory. Tried to allocate [1;36m768.00[0m MiB. GPU [1;36m0[0m has a total capacity of [1;36m4.00[0m GiB [2m                  [0m
[2;36m                      [0m         of which [1;36m0[0m bytes is free. Including non-PyTorch memory, this process has [1;36m17179869184.00[0m GiB memory in use. Of    [2m                  [0m
[2;36m                      [0m         the allocated memory [1;36m4.45[0m GiB is allocated by PyTorch, and [1;36m86.21[0m MiB is reserved by PyTorch but unallocated. If  [2m                  [0m
[2;36m                      [0m         reserved but unallocated memory is large try setting [33mPYTORCH_CUDA_ALLOC_CONF[0m=[35mexpandable_segments[0m:[3;92mTrue[0m to avoid   [2m                  [0m
[2;36m                      [0m         fragmentation.  See documentation for Memory Management                                                          [2m                  [0m
[2;36m                      [0m         [1m([0m[4;94mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables[0m[4;94m)[0m                                          [2m                  [0m
  0%|                                                                                                                                    | 0/1158 [00:03<?, ?it/s]
