  0%|                                                                                                                                            | 0/200 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████▍                                                                              | 80/200 [28:06<42:15, 21.13s/it]/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 18.3873, 'grad_norm': 1.7953851222991943, 'learning_rate': 0.0001752577319587629, 'epoch': 0.15}
{'loss': 16.425, 'grad_norm': 0.40442603826522827, 'learning_rate': 0.0001443298969072165, 'epoch': 0.3}
  return fn(*args, **kwargs)                                                                                                                                             
{'eval_loss': 1.9200265407562256, 'eval_runtime': 130.1496, 'eval_samples_per_second': 2.751, 'eval_steps_per_second': 1.375, 'epoch': 0.4}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████                          | 160/200 [56:29<11:22, 17.06s/it]/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 15.6197, 'grad_norm': 0.5497400164604187, 'learning_rate': 0.0001134020618556701, 'epoch': 0.45}
{'loss': 14.7944, 'grad_norm': 0.7219104170799255, 'learning_rate': 8.247422680412371e-05, 'epoch': 0.6}
{'loss': 14.214, 'grad_norm': 0.686867356300354, 'learning_rate': 5.1546391752577315e-05, 'epoch': 0.75}
  return fn(*args, **kwargs)                                                                                                                                             
{'eval_loss': 1.7711340188980103, 'eval_runtime': 133.6194, 'eval_samples_per_second': 2.679, 'eval_steps_per_second': 1.34, 'epoch': 0.8}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 181/200 [1:09:36<13:01, 41.11s/it]Traceback (most recent call last):
{'loss': 13.9131, 'grad_norm': 0.6584943532943726, 'learning_rate': 2.0618556701030927e-05, 'epoch': 0.9}
  File "/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py", line 286, in <module>
    train()
  File "/home/zahemen/projects/dl-lib/FinsightAI/src/main/train_qlora.py", line 261, in train
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zahemen/miniconda3/envs/transformer_LM/lib/python3.11/site-packages/transformers/trainer.py", line 2536, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
